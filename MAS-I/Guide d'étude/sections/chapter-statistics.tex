\chapter[Statistics]{Statistics (15\% à 30\%)}

\subsection{Information}

\begin{distributions}[Description]
Notes du descriptif principal:
\begin{itemize}
	\item	Topics which would commonly be covered in a 2-semester Probability \& Statistics sequence;
\end{itemize}
\end{distributions}

\begin{outcomes}[Learning objectives]
	\begin{enumerate}
	\item	Perform point estimation of statistical parameters using Maximum likelihood estimation (MLE). \\
			Apply criteria to estimates such as :
	\begin{multicols*}{2}
		\begin{itemize}
		\item	Consistency;
		\item	Unbiasedness;
		\item	Sufficiency;
		\item	Efficiency;
		\item	Minimum variance;
		\item	MSE;
		\end{itemize}
	\end{multicols*}
	\end{enumerate}
\end{outcomes}
\begin{knowledge}[Knowledge Statements]
	\begin{enumerate}[label = \alph*.]
	\item	Equations for MLE of mean, variance from a sample;
	\item	Estimation of mean and variance based on samples;
	\item	General equations for MLE of parameters;
	\item	Recognition of consistency property of estimators and alternative measures of consistency;
	\item	Application of criteria for measurement when estimating parameters through minimisation of variance, MSE;
	\item	Definition of statistical bias and recognition of estimators that are unbiased or biased;
	\item	Application of Rao-Cramer Lower Bound and Efficiency;
	\item	Relationship between Sufficiency and Minimum Variance;
	\item	Develop and estimate a sufficient statistic for a distribution;
	\item	Factorization Criterion for sufficiency;
	\item	Application of Rao-Cramer Lower Bound and Fisher Information;
	\item	Application of MVUE for the exponential class of distributions;
	\item	Linkage between Score Function, Fisher Information and maximum likelihood;
	\item	Method of Moments;
	\item	Percentile Matching;
	\item	Kernel Density Estimation;
	\item	Maximum Likelihood with Censoring and Truncation;
	\end{enumerate}
\end{knowledge}

\begin{outcomes}[Learning objectives]
	\begin{enumerate}
  \setcounter{enumi}{1}
	\item	Calculate parameter estimates using methods other than maximum likelihood.
	\item	Test statistical hypotheses including Type I and Type II errors using:
		\begin{itemize}
		\item	Neyman-Pearson theorem;
		\item[]	Apply Neyman-Pearson theorem to construct likelihood ratio equation;
		\item	Likelihood ratio tests;
		\item	First principles;
		\item[]	Use critical values from a sampling distribution to test means and variances;
		\end{itemize}	
	\end{enumerate}
\end{outcomes}	
\begin{knowledge}[Knowledge Statements]
	\begin{enumerate}[label = \alph*.]
	\item	Presentation of fundamental inequalities based on general assumptions and normal assumptions;
	\item	Definition of Type I and Type II errors;
	\item	Significance levels;
	\item	One-sided versus two-sided tests;
	\item	Estimation of sample sizes under normality to control for Type I and Type II errors;
	\item	Determination of critical regions;
	\item	Definition and measurement of likelihood ratio tests;
	\item	Determining parameters and testing using tabular values (from a table);
	\item	Recognizing when to apply likelihood ratio tests versus chi-square or other goodness of fit tests;
	\item	Apply paired $t$-test to two samples;
	\item	Test for difference in variance under Normal distribution between two samples through the application of $F$-test;
	\item	Test of significance of means from two samples under Normal distribution assumptions in both large and small sample cases;
	\item	Test for significance of difference in proportions between two samples under the Binomial distribution assumption in both large and small sample cases;
	\item	Application of contingency tables to test independence between effects;
	\item	Asymptotic relationship between likelihood ratio tests and the Chi-Square distribution;
	\item	Application of Neyman-Pearson theorem to Uniformly Most Powerful hypothesis tests;
	\item	Equivalence between critical regions and confidence intervals;
	\item	Kolmogorov-Smirnov test;
	\end{enumerate}
\end{knowledge}
	
\begin{outcomes}[Learning objectives]
	\begin{enumerate}
  \setcounter{enumi}{3}
	\item	For the Exponential, Gamma, Weibull, Pareto, Lognormal, Beta, and mixtures thereof:
		\begin{itemize}
		\item	Identify the applciations to Insurance claim modeling in which each distribution is used and reasons why;
		\item	Transformation of distributions; 
		\end{itemize}
	\end{enumerate}
\end{outcomes}
\begin{knowledge}[Knowledge Statements]
	\begin{enumerate}[label = \alph*.]
	\item	Frequency, severity and aggregate loss;
	\item	Common continuous distributions for modeling claim severity;
	\item	Mixing distributions;
	\item	Tail properties of claim severity;
	\item	Effects of coverage modifications including, for example: limits, deductibles, loss elimination ratios and effects of inflation;
	\end{enumerate}
\end{knowledge}

\begin{outcomes}[Learning objectives]
	\begin{enumerate}
  \setcounter{enumi}{4}
	\item	Calculate Order Statistics of a sample for a given distribution.
	\end{enumerate}
\end{outcomes}
\begin{knowledge}[Knowledge Statements]
	\begin{enumerate}[label = \alph*.]
	\item	General form for distribution of $n^{\text{th}}$ largest element of a set;
	\item	Application to a given distributional form;
	\end{enumerate}
\end{knowledge}

\begin{ASM_chapter}[Related lessons ASM]
\begin{enumerate}
  \setcounter{enumi}{24}
	\item	\nameref{L.-25}
	\item	\nameref{L.-26}
	\item	\nameref{L.-27}
	\item	\nameref{L.-28}
	\item	\nameref{L.-29}
	\item	\nameref{L.-30}
	\item	\nameref{L.-31}
	\item	\nameref{L.-32}
	\item	\nameref{L.-33}
	\item	\nameref{L.-34}
	\item	\nameref{L.-35}
	\item	\nameref{L.-36}
	\item	\nameref{L.-37}
	\item	\nameref{L.-38}
	\item	\nameref{L.-39}
	\item	\nameref{L.-40}
	\item	\nameref{L.-41}
\end{enumerate}
\end{ASM_chapter}

\begin{YTB_vids}[Vidéos YouTube]
\begin{itemize}
	\item	
\end{itemize}
\end{YTB_vids}

\begin{distributions}[Likely Questions]
\begin{itemize}
	\item	Question where we calculate the sample variance with the STAT function of the calculator;
		\begin{itemize}
		\item	MAS-I F19, \# 15	;
		\end{itemize}
\end{itemize}
\end{distributions}

\subsection{Résumés des chapitres}

\begin{CHPT_SUMM_AUTO_NUMB}[label = {L.-25}]{Estimator Quality}
Introduction to Mathematical Statistics 4.1.3, 5.1, 7.1

\tcbline

Overview of various basic functions to evaluate the quality of an estimator including:
\begin{enumerate}[leftmargin = *]
	\item	The Bias of an estimator (incl. asymptotically unbiased).
		\begin{itemize}[leftmargin = *]
		\item	Shows the sample mean $\bar{x}$ is an unbiased estimator of the true mean $\mu$.
		\item	Shows the sample variance $s^{2} = \frac{\sum (x_{i} - \bar{x})^{2}}{n - 1}$ is an unbiased estimator of the true variance $\sigma^{2}$.
		\item	Shows the empirical variance $\hat{\sigma}^{2} = \frac{\sum (x_{i} - \bar{x})^{2}}{n}$ is an biased estimator of the true variance $\sigma^{2}$.	\\
				However, with a bias of $\frac{n - 1}{n}\sigma^{2} - \sigma^{2} = -\frac{\sigma^{2}}{n}$, it is \textit{asymptotically} unbiased.
		\item	Gives the boiling water feet-freezer head analogy for why the bias is not sufficient to assess the quality of an estimator.
		\end{itemize}
	\item	The consistency of an estimator.
		\begin{itemize}[leftmargin = *]
		\item	As the number of observations $n$ on which an estimator $\hat{\theta}_{n}$ is based increases, if both the bias and variance of $\hat{\theta}_{n}$ go to 0, we can say it is a consistent estimator.	\\
				Although this condition is sufficient, it is not necessary.
		\item	For example, the sample mean $\bar{x}$ is a consistent estimator of the true mean for both the Gamma and Normal distributions.	\\
				However, a Pareto distribution with $\alpha \leq 2$ has an infinite variance; therefore, the variance of the estimator is infinite and $\bar{x}$ is not a consistent estimator.
		\end{itemize}
	\item	The efficiency of an estimator.
		\begin{itemize}[leftmargin = *]
		\item	Shows the relative efficiency of one estimator to another.
		\end{itemize}
	\item	The Mean Square Error (MSE) of an estimator.
		\begin{itemize}[leftmargin = *]
		\item	Shows both how it's defined as the variance of the estimator's predictions of the parameter and the relation with bias and variance.
		\end{itemize}
\end{enumerate}

We then combine these concepts to define the \textbf{uniformly minimum variance unbiased estimator (UMVUE)} which has a smaller variance, for any true value $\theta$, than any other \underline{unbiased} estimator.
\end{CHPT_SUMM_AUTO_NUMB}

\begin{CHPT_SUMM_AUTO_NUMB}[label = {L.-26}]{Kernel Density Estimation}
Nonlife Actuarial Models: Theory Methods and Evaluation 11.1
\tcbline
\begin{itemize}
		\item	
	\end{itemize}
\end{CHPT_SUMM_AUTO_NUMB}

\begin{CHPT_SUMM_AUTO_NUMB}[label = {L.-27}]{Method of Moments}
Nonlife Actuarial Models: Theory Methods and Evaluation 12.1.1

\tcbline

When we estimate the parameters for the underlying distribution, the question itself does not impact our estimates. 
\begin{itemize}[leftmargin = *]
	\item	For example, if we have data from a coverage with a policy limit of 100\$ and want to find the average payment per loss with a deductible of 100\$, then we estimate the parameters considering the policy limit and not the deductible.
	\item	This sounds obvious, but it is quite easy to make the mistake.
\end{itemize}

\tcbline

\begin{description}
	\item[Complete data]	Given the exact value of each observation.
\end{description}
Types of incomplete data:
\begin{description}
	\item[Grouped data]	Given a set of intervals and told how many observations are in each.
		\begin{itemize}[leftmargin = *]
		\item	For example:
		\end{itemize}
		\begin{center}
		\begin{tabular}{| >{\columncolor{beaublue}}c | >{\columncolor{beaublue}}c  |}
		\hline\rowcolor{airforceblue} 
		\textcolor{white}{\textbf{Range}}	&	\textcolor{white}{\textbf{Number of observations in range}}		\\
		0-999	&	85	\\\hline
		1000-1999	&	64	\\\hline
		2000 and over	&	72	\\\hline
		\end{tabular}
		\end{center}
	\item[Censored data]	Given that the value of an observation is in a range, but not given the exact value. 
		\begin{itemize}[leftmargin = *]
		\item	For example, a policy with a limit of $10\ 000\$$ ($Y = \min(X, 10\ 000)$).
		\end{itemize}
	\item[Truncated data]	Given the value of an observation only when it is in a certain range; typically, only above or below a certain number. 
		\begin{itemize}[leftmargin = *]
		\item	For example, a policy with a deductible of $100\$$ has no recorded losses of $100\$$ or less ($Y = \{X | X > 100\}$).
		\end{itemize}
\end{description}

\tcbline

To fit a $k$ parameter distribution, we set equal the $k$ first sample moments $\hat{\mu}_{k}$ to the $k$ first raw moments $\mu_{k}'$ of the distribution. \\	
We may match the variances instead of the second moments but we match the biased empirical variance  $\hat{\sigma}^{2}$ by default and not $s^{2}$.

\begin{align*}
	 \hat{\mu}_{j}	&=	 \mu_{j}', \; j = 1, \dots, k
\end{align*}
\tcbline

If the data is incomplete, we match the moments of the corresponding distribution.
\begin{itemize}[leftmargin = *]
	\item	If data is censored at $u$, $\hat{\mu}_{k}	=	 \text{E}[\min(X; u)^{k}]$.
	\item	If data is truncated at $d$, $\hat{\mu}_{k}	=	 \text{E}[X^{k} | X > d]$.
\end{itemize}
\end{CHPT_SUMM_AUTO_NUMB}

\begin{CHPT_SUMM_AUTO_NUMB}[label = {L.-28}]{Percentile Matching}
Nonlife Actuarial Models: Theory Methods and Evaluation 11.1.1, 12.1.2

\tcbline

\begin{algo}{Smoothed empirical estimate}
We use the sample's order statistics $x_{(1)} \le x_{(2)} \le \dots \le x_{(n)}$ for the following interpolation:
\begin{align*}
	\hat\pi_{g}
	&=	(1 - h)x_{(j)} + h x_{(j + 1)}, \quad \text{ where }	\\
	j
	&=	\lfloor (n + 1) g \rfloor	\quad
	\text{ and }	 \quad
	h
	=	(n + 1) g - j
\end{align*}
\end{algo}
\begin{itemize}[leftmargin = *]
	\item	Exams should specify which percentiles to use.
	\item	Percentile matching itself matches $F(\hat{\pi}_{g})	=	g$.
	\item	Exams don't typically ask a lot of percentile matching questions, thus it's not really worth memorizing each distribution's formulas as it can easily be about a random distribution.
\end{itemize}

\tcbline

\begin{itemize}[leftmargin = *]
	\item	With censored data, the only additional consideration is that the percentile must be within the range of the uncensored portion of the data.
	\item	With truncated data, we match the percentile of the conditional distribution $(X | X > d)$.
\end{itemize}
\end{CHPT_SUMM_AUTO_NUMB}

\begin{CHPT_SUMM_AUTO_NUMB}[label = {L.-29}]{Maximum Likelihood Estimators}
Introduction to Mathematical Statistics 4.1, 6.1\\
Nonlife Actuarial Models: Theory Methods and Evaluation 10.2, 12.3

\tcbline

Whatever the type of data, maximise the function representing the probability of the observation occuring \lfbox[formula]{$\mathcal{L}(\theta)	=	\prod g(x_{i}, \theta)$}.\\

Therefore $g(x_{i}, \theta)	=$
\begin{description}
	\item[$f(x_{i})$]	for complete individual observations.
	\item[$F(x_{i})	-	F(x_{i	-	1})$]	for grouped observations.
	\item[$S(u)$]	for observations censored at $u$ (for those below $u$, $g(x_{i}, \theta)	=	f(x_{i})$).
	\item[$\frac{f(x_{i})}{S(d)}$]	for observations truncated at $d$.
\end{description}
\end{CHPT_SUMM_AUTO_NUMB}

\begin{CHPT_SUMM_AUTO_NUMB}[label = {L.-30}]{Maximum Likelihood Estimators---Special Techniques}
Introduction to Mathematical Statistics 4.1, 6.1	\\
Nonlife Actuarial Models: Theory Methods and Evaluation 10.2, 12.3

\tcbline

	\begin{itemize}
		\item	
	\end{itemize}
\end{CHPT_SUMM_AUTO_NUMB}

\begin{CHPT_SUMM_AUTO_NUMB}[label = {L.-31}]{Variance of Maximum Likelihood Estimator}
Introduction to Mathematical Statistics 6.2, 6.5

\tcbline


\end{CHPT_SUMM_AUTO_NUMB}

\begin{CHPT_SUMM_AUTO_NUMB}[label = {L.-32}]{Sufficient Statistics}
Introduction to Mathematical Statistics 7

\tcbline


\end{CHPT_SUMM_AUTO_NUMB}

\begin{CHPT_SUMM_AUTO_NUMB}[label = {L.-33}]{Hypothesis Testing}
Introduction to Mathematical Statistics 4.5, 4.6

\tcbline


\end{CHPT_SUMM_AUTO_NUMB}

\begin{CHPT_SUMM_AUTO_NUMB}[label = {L.-34}]{Confidence Intervals and Sample Size}
Introduction to Mathematical Statistics 4.5, 4.6

\tcbline


\end{CHPT_SUMM_AUTO_NUMB}

\begin{CHPT_SUMM_AUTO_NUMB}[label = {L.-35}]{Confidence Intervals for Means}
Introduction to Mathematical Statistics 4.2

\tcbline


\end{CHPT_SUMM_AUTO_NUMB}

\begin{CHPT_SUMM_AUTO_NUMB}[label = {L.-36}]{Kolmogorov-Smirnov Tests}
Nonlife Actuarial Models: Theory Methods and Evaluation 13.2.1

\tcbline


\end{CHPT_SUMM_AUTO_NUMB}

\begin{CHPT_SUMM_AUTO_NUMB}[label = {L.-37}]{Chi Square Tests}
Introduction to Mathematical 4.7	\\
Nonlife Actuarial Models: Theory Methods and Evaluation 13.2.3

\tcbline


\end{CHPT_SUMM_AUTO_NUMB}

\begin{CHPT_SUMM_AUTO_NUMB}[label = {L.-38}]{Confidence Intervals for Variances}
Introduction to Mathematical Statistics 8.3

\tcbline


\end{CHPT_SUMM_AUTO_NUMB}

\begin{CHPT_SUMM_AUTO_NUMB}[label = {L.-39}]{Uniformly Most Powerful Critical Regions}
Introduction to Mathematical Statistics 8.1 - 8.2

\tcbline


\end{CHPT_SUMM_AUTO_NUMB}

\begin{CHPT_SUMM_AUTO_NUMB}[label = {L.-40}]{Likelihood Ratio Tests}
Introduction to Mathematical Statistics 8.3

\tcbline


\end{CHPT_SUMM_AUTO_NUMB}

\begin{CHPT_SUMM_AUTO_NUMB}[label = {L.-41}]{Q.-Q. Plots}
Introduction to Mathematical Statistics 4.4	\\
Larsen Study Note

\tcbline


\end{CHPT_SUMM_AUTO_NUMB}

\subsection{Notes sur les vidéos YouTube}

\begin{YTB_SUMM}[label = {SQ-BASICS-ML-INTRO}]{\href{https://www.youtube.com/watch?v=Gv9_4yMHFhI&list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF&index=2&t=0s}{StatQuest: A Gentle Introduction to Machine Learning}}
\begin{itemize}
	\item	
\end{itemize}
\end{YTB_SUMM}
